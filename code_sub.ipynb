{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "import random,string\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "import random,string\n",
    "import datetime\n",
    "import scipy.sparse as sp\n",
    "\n",
    "import pickle as pkl\n",
    "import sys\n",
    "import networkx as nx\n",
    "\n",
    "import math\n",
    "\n",
    "import torch\n",
    "\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn.modules.module import Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalized_laplacian(adj):\n",
    "   adj = sp.coo_matrix(adj)\n",
    "   row_sum = np.array(adj.sum(1))\n",
    "   d_inv_sqrt = np.power(row_sum, -0.5).flatten()\n",
    "   d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n",
    "   d_mat_inv_sqrt = sp.diags(d_inv_sqrt)\n",
    "   return (sp.eye(adj.shape[0]) - d_mat_inv_sqrt.dot(adj).dot(d_mat_inv_sqrt)).tocoo()\n",
    "\n",
    "\n",
    "def laplacian(adj):\n",
    "   adj = sp.coo_matrix(adj)\n",
    "   row_sum = np.array(adj.sum(1)).flatten()\n",
    "   d_mat = sp.diags(row_sum)\n",
    "   return (d_mat - adj).tocoo()\n",
    "\n",
    "\n",
    "def gcn(adj):\n",
    "   adj = sp.coo_matrix(adj)\n",
    "   row_sum = np.array(adj.sum(1))\n",
    "   d_inv_sqrt = np.power(row_sum, -0.5).flatten()\n",
    "   d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n",
    "   d_mat_inv_sqrt = sp.diags(d_inv_sqrt)\n",
    "   return (sp.eye(adj.shape[0]) + d_mat_inv_sqrt.dot(adj).dot(d_mat_inv_sqrt)).tocoo()\n",
    "\n",
    "\n",
    "def aug_normalized_adjacency(adj):\n",
    "   adj = adj + sp.eye(adj.shape[0])\n",
    "   adj = sp.coo_matrix(adj)\n",
    "   row_sum = np.array(adj.sum(1))\n",
    "   d_inv_sqrt = np.power(row_sum, -0.5).flatten()\n",
    "   d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n",
    "   d_mat_inv_sqrt = sp.diags(d_inv_sqrt)\n",
    "   return d_mat_inv_sqrt.dot(adj).dot(d_mat_inv_sqrt).tocoo()\n",
    "\n",
    "def bingge_norm_adjacency(adj):\n",
    "   adj = adj + sp.eye(adj.shape[0])\n",
    "   adj = sp.coo_matrix(adj)\n",
    "   row_sum = np.array(adj.sum(1))\n",
    "   d_inv_sqrt = np.power(row_sum, -0.5).flatten()\n",
    "   d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n",
    "   d_mat_inv_sqrt = sp.diags(d_inv_sqrt)\n",
    "   return (d_mat_inv_sqrt.dot(adj).dot(d_mat_inv_sqrt) +  sp.eye(adj.shape[0])).tocoo()\n",
    "\n",
    "def normalized_adjacency(adj):\n",
    "   adj = sp.coo_matrix(adj)\n",
    "   row_sum = np.array(adj.sum(1))\n",
    "   d_inv_sqrt = np.power(row_sum, -0.5).flatten()\n",
    "   d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n",
    "   d_mat_inv_sqrt = sp.diags(d_inv_sqrt)\n",
    "   return (d_mat_inv_sqrt.dot(adj).dot(d_mat_inv_sqrt)).tocoo()\n",
    "\n",
    "def random_walk_laplacian(adj):\n",
    "   adj = sp.coo_matrix(adj)\n",
    "   row_sum = np.array(adj.sum(1))\n",
    "   d_inv = np.power(row_sum, -1.0).flatten()\n",
    "   d_mat = sp.diags(d_inv)\n",
    "   return (sp.eye(adj.shape[0]) - d_mat.dot(adj)).tocoo()\n",
    "\n",
    "\n",
    "def aug_random_walk(adj):\n",
    "   adj = adj + sp.eye(adj.shape[0])\n",
    "   adj = sp.coo_matrix(adj)\n",
    "   row_sum = np.array(adj.sum(1))\n",
    "   d_inv = np.power(row_sum, -1.0).flatten()\n",
    "   d_mat = sp.diags(d_inv)\n",
    "   return (d_mat.dot(adj)).tocoo()\n",
    "\n",
    "def random_walk(adj):\n",
    "   adj = sp.coo_matrix(adj)\n",
    "   row_sum = np.array(adj.sum(1))\n",
    "   d_inv = np.power(row_sum, -1.0).flatten()\n",
    "   d_mat = sp.diags(d_inv)\n",
    "   return d_mat.dot(adj).tocoo()\n",
    "\n",
    "def no_norm(adj):\n",
    "   adj = sp.coo_matrix(adj)\n",
    "   return adj\n",
    "\n",
    "\n",
    "def i_norm(adj):\n",
    "    adj = adj + sp.eye(adj.shape[0])\n",
    "    adj = sp.coo_matrix(adj)\n",
    "    return adj\n",
    "  \n",
    "def fetch_normalization(type):\n",
    "   switcher = {\n",
    "       'NormLap': normalized_laplacian,  # A' = I - D^-1/2 * A * D^-1/2\n",
    "       'Lap': laplacian,  # A' = D - A\n",
    "       'RWalkLap': random_walk_laplacian,  # A' = I - D^-1 * A\n",
    "       'FirstOrderGCN': gcn,   # A' = I + D^-1/2 * A * D^-1/2\n",
    "       'AugNormAdj': aug_normalized_adjacency,  # A' = (D + I)^-1/2 * ( A + I ) * (D + I)^-1/2\n",
    "       'BingGeNormAdj': bingge_norm_adjacency, # A' = I + (D + I)^-1/2 * (A + I) * (D + I)^-1/2\n",
    "       'NormAdj': normalized_adjacency,  # D^-1/2 * A * D^-1/2\n",
    "       'RWalk': random_walk,  # A' = D^-1*A\n",
    "       'AugRWalk': aug_random_walk,  # A' = (D + I)^-1*(A + I)\n",
    "       'NoNorm': no_norm, # A' = A\n",
    "       'INorm': i_norm,  # A' = A + I\n",
    "   }\n",
    "   func = switcher.get(type, lambda: \"Invalid normalization technique.\")\n",
    "   return func\n",
    "\n",
    "def row_normalize(mx):\n",
    "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
    "    rowsum = np.array(mx.sum(1))\n",
    "    r_inv = np.power(rowsum, -1).flatten()\n",
    "    r_inv[np.isinf(r_inv)] = 0.\n",
    "    r_mat_inv = sp.diags(r_inv)\n",
    "    mx = r_mat_inv.dot(mx)\n",
    "    return mx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir = \"data\"\n",
    "\n",
    "def parse_index_file(filename):\n",
    "    \"\"\"Parse index file.\"\"\"\n",
    "    index = []\n",
    "    for line in open(filename):\n",
    "        index.append(int(line.strip()))\n",
    "    return index\n",
    "\n",
    "def preprocess_citation(adj, features, normalization=\"FirstOrderGCN\"):\n",
    "    adj_normalizer = fetch_normalization(normalization)\n",
    "    adj = adj_normalizer(adj)\n",
    "    features = row_normalize(features)\n",
    "    return adj, features\n",
    "\n",
    "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
    "    \"\"\"Convert a scipy sparse matrix to a torch sparse tensor.\"\"\"\n",
    "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
    "    indices = torch.from_numpy(\n",
    "        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
    "    values = torch.from_numpy(sparse_mx.data)\n",
    "    shape = torch.Size(sparse_mx.shape)\n",
    "    return torch.sparse.FloatTensor(indices, values, shape)\n",
    "\n",
    "\n",
    "\n",
    "def load_citation(dataset_str=\"cora\", normalization=\"AugNormAdj\", porting_to_torch=True,data_path=datadir, task_type=\"full\"):\n",
    "    \"\"\"\n",
    "    Load Citation Networks Datasets.\n",
    "    \"\"\"\n",
    "    names = ['x', 'y', 'tx', 'ty', 'allx', 'ally', 'graph']\n",
    "    objects = []\n",
    "    for i in range(len(names)):\n",
    "        with open(os.path.join(data_path, \"ind.{}.{}\".format(dataset_str.lower(), names[i])), 'rb') as f:\n",
    "            if sys.version_info > (3, 0):\n",
    "                objects.append(pkl.load(f, encoding='latin1'))\n",
    "            else:\n",
    "                objects.append(pkl.load(f))\n",
    "\n",
    "    x, y, tx, ty, allx, ally, graph = tuple(objects)\n",
    "    test_idx_reorder = parse_index_file(os.path.join(data_path, \"ind.{}.test.index\".format(dataset_str)))\n",
    "    test_idx_range = np.sort(test_idx_reorder)\n",
    "\n",
    "    if dataset_str == 'citeseer':\n",
    "        # Fix citeseer dataset (there are some isolated nodes in the graph)\n",
    "        # Find isolated nodes, add them as zero-vecs into the right position\n",
    "        test_idx_range_full = range(min(test_idx_reorder), max(test_idx_reorder)+1)\n",
    "        tx_extended = sp.lil_matrix((len(test_idx_range_full), x.shape[1]))\n",
    "        tx_extended[test_idx_range-min(test_idx_range), :] = tx\n",
    "        tx = tx_extended\n",
    "        ty_extended = np.zeros((len(test_idx_range_full), y.shape[1]))\n",
    "        ty_extended[test_idx_range-min(test_idx_range), :] = ty\n",
    "        ty = ty_extended\n",
    "\n",
    "    features = sp.vstack((allx, tx)).tolil()\n",
    "    features[test_idx_reorder, :] = features[test_idx_range, :]\n",
    "    G = nx.from_dict_of_lists(graph)\n",
    "    adj = nx.adjacency_matrix(G)\n",
    "    adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
    "    # degree = np.asarray(G.degree)\n",
    "    degree = np.sum(adj, axis=1)\n",
    "\n",
    "    labels = np.vstack((ally, ty))\n",
    "    labels[test_idx_reorder, :] = labels[test_idx_range, :]\n",
    "    \n",
    "    if task_type == \"full\":\n",
    "        print(\"Load full supervised task.\")\n",
    "        #supervised setting\n",
    "        idx_test = test_idx_range.tolist()\n",
    "        idx_train = range(len(ally)- 500)\n",
    "        idx_val = range(len(ally) - 500, len(ally))\n",
    "    elif task_type == \"semi\":\n",
    "        print(\"Load semi-supervised task.\")\n",
    "        #semi-supervised setting\n",
    "        idx_test = test_idx_range.tolist()\n",
    "        idx_train = range(len(y))\n",
    "        idx_val = range(len(y), len(y)+500)\n",
    "    else:\n",
    "        raise ValueError(\"Task type: %s is not supported. Available option: full and semi.\")\n",
    "\n",
    "    adj, features = preprocess_citation(adj, features, normalization)\n",
    "    features = np.array(features.todense())\n",
    "    labels = np.argmax(labels, axis=1)\n",
    "    # porting to pytorch\n",
    "    if porting_to_torch:\n",
    "        features = torch.FloatTensor(features).float()\n",
    "        labels = torch.LongTensor(labels)\n",
    "        # labels = torch.max(labels, dim=1)[1]\n",
    "        adj = sparse_mx_to_torch_sparse_tensor(adj).float()\n",
    "        idx_train = torch.LongTensor(idx_train)\n",
    "        idx_val = torch.LongTensor(idx_val)\n",
    "        idx_test = torch.LongTensor(idx_test)\n",
    "        degree = torch.LongTensor(degree)\n",
    "    learning_type = \"transductive\"\n",
    "    return adj, features, labels, idx_train, idx_val, idx_test, degree, learning_type\n",
    "\n",
    "def sgc_precompute(features, adj, degree):\n",
    "    #t = perf_counter()\n",
    "    for i in range(degree):\n",
    "        features = torch.spmm(adj, features)\n",
    "    precompute_time = 0 #perf_counter()-t\n",
    "    return features, precompute_time\n",
    "\n",
    "def set_seed(seed, cuda):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if cuda: torch.cuda.manual_seed(seed)\n",
    "\n",
    "\n",
    "def loadRedditFromNPZ(dataset_dir=datadir):\n",
    "    adj = sp.load_npz(dataset_dir+\"reddit_adj.npz\")\n",
    "    data = np.load(dataset_dir +\"reddit.npz\")\n",
    "\n",
    "    return adj, data['feats'], data['y_train'], data['y_val'], data['y_test'], data['train_index'], data['val_index'], data['test_index']\n",
    "\n",
    "\n",
    "def load_reddit_data(normalization=\"AugNormAdj\", porting_to_torch=True, data_path=datadir):\n",
    "    adj, features, y_train, y_val, y_test, train_index, val_index, test_index = loadRedditFromNPZ(data_path)\n",
    "    labels = np.zeros(adj.shape[0])\n",
    "    labels[train_index]  = y_train\n",
    "    labels[val_index]  = y_val\n",
    "    labels[test_index]  = y_test\n",
    "    adj = adj + adj.T + sp.eye(adj.shape[0])\n",
    "    train_adj = adj[train_index, :][:, train_index]\n",
    "    degree = np.sum(train_adj, axis=1)\n",
    "\n",
    "    features = torch.FloatTensor(np.array(features))\n",
    "    features = (features-features.mean(dim=0))/features.std(dim=0)\n",
    "    train_features = torch.index_select(features, 0, torch.LongTensor(train_index))\n",
    "    if not porting_to_torch:\n",
    "        features = features.numpy()\n",
    "        train_features = train_features.numpy()\n",
    "\n",
    "    adj_normalizer = fetch_normalization(normalization)\n",
    "    adj = adj_normalizer(adj)\n",
    "    train_adj = adj_normalizer(train_adj)\n",
    "\n",
    "    if porting_to_torch:\n",
    "        train_adj = sparse_mx_to_torch_sparse_tensor(train_adj).float()\n",
    "        labels = torch.LongTensor(labels)\n",
    "        adj = sparse_mx_to_torch_sparse_tensor(adj).float()\n",
    "        degree = torch.LongTensor(degree)\n",
    "        train_index = torch.LongTensor(train_index)\n",
    "        val_index = torch.LongTensor(val_index)\n",
    "        test_index = torch.LongTensor(test_index)\n",
    "    learning_type = \"inductive\"\n",
    "    return adj, train_adj, features, train_features, labels, train_index, val_index, test_index, degree, learning_type\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "def data_loader(dataset, data_path=datadir, normalization=\"AugNormAdj\", porting_to_torch=True, task_type = \"full\"):\n",
    "    if dataset == \"reddit\":\n",
    "        return load_reddit_data(normalization, porting_to_torch, data_path)\n",
    "    else:\n",
    "        (adj,\n",
    "         features,\n",
    "         labels,\n",
    "         idx_train,\n",
    "         idx_val,\n",
    "         idx_test,\n",
    "         degree,\n",
    "         learning_type) = load_citation(dataset, normalization, porting_to_torch, data_path, task_type)\n",
    "        train_adj = adj\n",
    "        train_features = features\n",
    "        return adj, train_adj, features, train_features, labels, idx_train, idx_val, idx_test, degree, learning_type\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class GraphConvolutionBS(Module):\n",
    "    \"\"\"\n",
    "    GCN Layer with BN, Self-loop and Res connection.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, activation=lambda x: x, withbn=True, withloop=True, bias=True,\n",
    "                 res=False):\n",
    "        \"\"\"\n",
    "        Initial function.\n",
    "        :param in_features: the input feature dimension.\n",
    "        :param out_features: the output feature dimension.\n",
    "        :param activation: the activation function.\n",
    "        :param withbn: using batch normalization.\n",
    "        :param withloop: using self feature modeling.\n",
    "        :param bias: enable bias.\n",
    "        :param res: enable res connections.\n",
    "        \"\"\"\n",
    "        super(GraphConvolutionBS, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.sigma = activation\n",
    "        self.res = res\n",
    "\n",
    "        # Parameter setting.\n",
    "        self.weight = Parameter(torch.FloatTensor(in_features, out_features))\n",
    "        # Is this the best practice or not?\n",
    "        if withloop:\n",
    "            self.self_weight = Parameter(torch.FloatTensor(in_features, out_features))\n",
    "        else:\n",
    "            self.register_parameter(\"self_weight\", None)\n",
    "\n",
    "        if withbn:\n",
    "            self.bn = torch.nn.BatchNorm1d(out_features)\n",
    "        else:\n",
    "            self.register_parameter(\"bn\", None)\n",
    "\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.FloatTensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        if self.self_weight is not None:\n",
    "            stdv = 1. / math.sqrt(self.self_weight.size(1))\n",
    "            self.self_weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, input, adj):\n",
    "        support = torch.mm(input, self.weight)\n",
    "        output = torch.spmm(adj, support)\n",
    "\n",
    "        # Self-loop\n",
    "        if self.self_weight is not None:\n",
    "            output = output + torch.mm(input, self.self_weight)\n",
    "\n",
    "        if self.bias is not None:\n",
    "            output = output + self.bias\n",
    "        # BN\n",
    "        if self.bn is not None:\n",
    "            output = self.bn(output)\n",
    "        # Res\n",
    "        if self.res:\n",
    "            return self.sigma(output) + input\n",
    "        else:\n",
    "            return self.sigma(output)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' \\\n",
    "               + str(self.in_features) + ' -> ' \\\n",
    "               + str(self.out_features) + ')'\n",
    "\n",
    "class GraphBaseBlock(Module):\n",
    "    \"\"\"\n",
    "    The base block for Multi-layer GCN / ResGCN / Dense GCN \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, nbaselayer,\n",
    "                 withbn=True, withloop=True, activation=F.relu, dropout=True,\n",
    "                 aggrmethod=\"concat\", dense=False):\n",
    "        \"\"\"\n",
    "        The base block for constructing DeepGCN model.\n",
    "        :param in_features: the input feature dimension.\n",
    "        :param out_features: the hidden feature dimension.\n",
    "        :param nbaselayer: the number of layers in the base block.\n",
    "        :param withbn: using batch normalization in graph convolution.\n",
    "        :param withloop: using self feature modeling in graph convolution.\n",
    "        :param activation: the activation function, default is ReLu.\n",
    "        :param dropout: the dropout ratio.\n",
    "        :param aggrmethod: the aggregation function for baseblock, can be \"concat\" and \"add\". For \"resgcn\", the default\n",
    "                           is \"add\", for others the default is \"concat\".\n",
    "        :param dense: enable dense connection\n",
    "        \"\"\"\n",
    "        super(GraphBaseBlock, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.hiddendim = out_features\n",
    "        self.nhiddenlayer = nbaselayer\n",
    "        self.activation = activation\n",
    "        self.aggrmethod = aggrmethod\n",
    "        self.dense = dense\n",
    "        self.dropout = dropout\n",
    "        self.withbn = withbn\n",
    "        self.withloop = withloop\n",
    "        self.hiddenlayers = nn.ModuleList()\n",
    "        self.__makehidden()\n",
    "\n",
    "        if self.aggrmethod == \"concat\" and dense == False:\n",
    "            self.out_features = in_features + out_features\n",
    "        elif self.aggrmethod == \"concat\" and dense == True:\n",
    "            self.out_features = in_features + out_features * nbaselayer\n",
    "        elif self.aggrmethod == \"add\":\n",
    "            if in_features != self.hiddendim:\n",
    "                raise RuntimeError(\"The dimension of in_features and hiddendim should be matched in add model.\")\n",
    "            self.out_features = out_features\n",
    "        elif self.aggrmethod == \"nores\":\n",
    "            self.out_features = out_features\n",
    "        else:\n",
    "            raise NotImplementedError(\"The aggregation method only support 'concat','add' and 'nores'.\")\n",
    "\n",
    "    def __makehidden(self):\n",
    "        # for i in xrange(self.nhiddenlayer):\n",
    "        for i in range(self.nhiddenlayer):\n",
    "            if i == 0:\n",
    "                layer = GraphConvolutionBS(self.in_features, self.hiddendim, self.activation, self.withbn,\n",
    "                                           self.withloop)\n",
    "            else:\n",
    "                layer = GraphConvolutionBS(self.hiddendim, self.hiddendim, self.activation, self.withbn, self.withloop)\n",
    "            self.hiddenlayers.append(layer)\n",
    "\n",
    "    def _doconcat(self, x, subx):\n",
    "        if x is None:\n",
    "            return subx\n",
    "        if self.aggrmethod == \"concat\":\n",
    "            return torch.cat((x, subx), 1)\n",
    "        elif self.aggrmethod == \"add\":\n",
    "            return x + subx\n",
    "        elif self.aggrmethod == \"nores\":\n",
    "            return x\n",
    "\n",
    "    def forward(self, input, adj):\n",
    "        x = input\n",
    "        denseout = None\n",
    "        # Here out is the result in all levels.\n",
    "        for gc in self.hiddenlayers:\n",
    "            denseout = self._doconcat(denseout, x)\n",
    "            x = gc(x, adj)\n",
    "            x = F.dropout(x, self.dropout, training=self.training)\n",
    "\n",
    "        if not self.dense:\n",
    "            return self._doconcat(x, input)\n",
    "        return self._doconcat(x, denseout)\n",
    "\n",
    "    def get_outdim(self):\n",
    "        return self.out_features\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"%s %s (%d - [%d:%d] > %d)\" % (self.__class__.__name__,\n",
    "                                              self.aggrmethod,\n",
    "                                              self.in_features,\n",
    "                                              self.hiddendim,\n",
    "                                              self.nhiddenlayer,\n",
    "                                              self.out_features)\n",
    "\n",
    "\n",
    "class MultiLayerGCNBlock(Module):\n",
    "    \"\"\"\n",
    "    Muti-Layer GCN with same hidden dimension.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, nbaselayer,\n",
    "                 withbn=True, withloop=True, activation=F.relu, dropout=True,\n",
    "                 aggrmethod=None, dense=None):\n",
    "        \"\"\"\n",
    "        The multiple layer GCN block.\n",
    "        :param in_features: the input feature dimension.\n",
    "        :param out_features: the hidden feature dimension.\n",
    "        :param nbaselayer: the number of layers in the base block.\n",
    "        :param withbn: using batch normalization in graph convolution.\n",
    "        :param withloop: using self feature modeling in graph convolution.\n",
    "        :param activation: the activation function, default is ReLu.\n",
    "        :param dropout: the dropout ratio.\n",
    "        :param aggrmethod: not applied.\n",
    "        :param dense: not applied.\n",
    "        \"\"\"\n",
    "        super(MultiLayerGCNBlock, self).__init__()\n",
    "        self.model = GraphBaseBlock(in_features=in_features,\n",
    "                                    out_features=out_features,\n",
    "                                    nbaselayer=nbaselayer,\n",
    "                                    withbn=withbn,\n",
    "                                    withloop=withloop,\n",
    "                                    activation=activation,\n",
    "                                    dropout=dropout,\n",
    "                                    dense=False,\n",
    "                                    aggrmethod=\"nores\")\n",
    "\n",
    "    def forward(self, input, adj):\n",
    "        return self.model.forward(input, adj)\n",
    "\n",
    "    def get_outdim(self):\n",
    "        return self.model.get_outdim()\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"%s %s (%d - [%d:%d] > %d)\" % (self.__class__.__name__,\n",
    "                                              self.aggrmethod,\n",
    "                                              self.model.in_features,\n",
    "                                              self.model.hiddendim,\n",
    "                                              self.model.nhiddenlayer,\n",
    "                                              self.model.out_features)\n",
    "\n",
    "\n",
    "class ResGCNBlock(Module):\n",
    "    \"\"\"\n",
    "    The multiple layer GCN with residual connection block.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, nbaselayer,\n",
    "                 withbn=True, withloop=True, activation=F.relu, dropout=True,\n",
    "                 aggrmethod=None, dense=None):\n",
    "        \"\"\"\n",
    "        The multiple layer GCN with residual connection block.\n",
    "        :param in_features: the input feature dimension.\n",
    "        :param out_features: the hidden feature dimension.\n",
    "        :param nbaselayer: the number of layers in the base block.\n",
    "        :param withbn: using batch normalization in graph convolution.\n",
    "        :param withloop: using self feature modeling in graph convolution.\n",
    "        :param activation: the activation function, default is ReLu.\n",
    "        :param dropout: the dropout ratio.\n",
    "        :param aggrmethod: not applied.\n",
    "        :param dense: not applied.\n",
    "        \"\"\"\n",
    "        super(ResGCNBlock, self).__init__()\n",
    "        self.model = GraphBaseBlock(in_features=in_features,\n",
    "                                    out_features=out_features,\n",
    "                                    nbaselayer=nbaselayer,\n",
    "                                    withbn=withbn,\n",
    "                                    withloop=withloop,\n",
    "                                    activation=activation,\n",
    "                                    dropout=dropout,\n",
    "                                    dense=False,\n",
    "                                    aggrmethod=\"add\")\n",
    "\n",
    "    def forward(self, input, adj):\n",
    "        return self.model.forward(input, adj)\n",
    "\n",
    "    def get_outdim(self):\n",
    "        return self.model.get_outdim()\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"%s %s (%d - [%d:%d] > %d)\" % (self.__class__.__name__,\n",
    "                                              self.aggrmethod,\n",
    "                                              self.model.in_features,\n",
    "                                              self.model.hiddendim,\n",
    "                                              self.model.nhiddenlayer,\n",
    "                                              self.model.out_features)\n",
    "\n",
    "\n",
    "class DenseGCNBlock(Module):\n",
    "    \"\"\"\n",
    "    The multiple layer GCN with dense connection block.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, nbaselayer,\n",
    "                 withbn=True, withloop=True, activation=F.relu, dropout=True,\n",
    "                 aggrmethod=\"concat\", dense=True):\n",
    "        \"\"\"\n",
    "        The multiple layer GCN with dense connection block.\n",
    "        :param in_features: the input feature dimension.\n",
    "        :param out_features: the hidden feature dimension.\n",
    "        :param nbaselayer: the number of layers in the base block.\n",
    "        :param withbn: using batch normalization in graph convolution.\n",
    "        :param withloop: using self feature modeling in graph convolution.\n",
    "        :param activation: the activation function, default is ReLu.\n",
    "        :param dropout: the dropout ratio.\n",
    "        :param aggrmethod: the aggregation function for the output. For denseblock, default is \"concat\".\n",
    "        :param dense: default is True, cannot be changed.\n",
    "        \"\"\"\n",
    "        super(DenseGCNBlock, self).__init__()\n",
    "        self.model = GraphBaseBlock(in_features=in_features,\n",
    "                                    out_features=out_features,\n",
    "                                    nbaselayer=nbaselayer,\n",
    "                                    withbn=withbn,\n",
    "                                    withloop=withloop,\n",
    "                                    activation=activation,\n",
    "                                    dropout=dropout,\n",
    "                                    dense=True,\n",
    "                                    aggrmethod=aggrmethod)\n",
    "\n",
    "    def forward(self, input, adj):\n",
    "        return self.model.forward(input, adj)\n",
    "\n",
    "    def get_outdim(self):\n",
    "        return self.model.get_outdim()\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"%s %s (%d - [%d:%d] > %d)\" % (self.__class__.__name__,\n",
    "                                              self.aggrmethod,\n",
    "                                              self.model.in_features,\n",
    "                                              self.model.hiddendim,\n",
    "                                              self.model.nhiddenlayer,\n",
    "                                              self.model.out_features)\n",
    "\n",
    "\n",
    "class InecptionGCNBlock(Module):\n",
    "    \"\"\"\n",
    "    The multiple layer GCN with inception connection block.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, nbaselayer,\n",
    "                 withbn=True, withloop=True, activation=F.relu, dropout=True,\n",
    "                 aggrmethod=\"concat\", dense=False):\n",
    "        \"\"\"\n",
    "        The multiple layer GCN with inception connection block.\n",
    "        :param in_features: the input feature dimension.\n",
    "        :param out_features: the hidden feature dimension.\n",
    "        :param nbaselayer: the number of layers in the base block.\n",
    "        :param withbn: using batch normalization in graph convolution.\n",
    "        :param withloop: using self feature modeling in graph convolution.\n",
    "        :param activation: the activation function, default is ReLu.\n",
    "        :param dropout: the dropout ratio.\n",
    "        :param aggrmethod: the aggregation function for baseblock, can be \"concat\" and \"add\". For \"resgcn\", the default\n",
    "                           is \"add\", for others the default is \"concat\".\n",
    "        :param dense: not applied. The default is False, cannot be changed.\n",
    "        \"\"\"\n",
    "        super(InecptionGCNBlock, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.hiddendim = out_features\n",
    "        self.nbaselayer = nbaselayer\n",
    "        self.activation = activation\n",
    "        self.aggrmethod = aggrmethod\n",
    "        self.dropout = dropout\n",
    "        self.withbn = withbn\n",
    "        self.withloop = withloop\n",
    "        self.midlayers = nn.ModuleList()\n",
    "        self.__makehidden()\n",
    "\n",
    "        if self.aggrmethod == \"concat\":\n",
    "            self.out_features = in_features + out_features * nbaselayer\n",
    "        elif self.aggrmethod == \"add\":\n",
    "            if in_features != self.hiddendim:\n",
    "                raise RuntimeError(\"The dimension of in_features and hiddendim should be matched in 'add' model.\")\n",
    "            self.out_features = out_features\n",
    "        else:\n",
    "            raise NotImplementedError(\"The aggregation method only support 'concat', 'add'.\")\n",
    "\n",
    "    def __makehidden(self):\n",
    "        # for j in xrange(self.nhiddenlayer):\n",
    "        for j in range(self.nbaselayer):\n",
    "            reslayer = nn.ModuleList()\n",
    "            # for i in xrange(j + 1):\n",
    "            for i in range(j + 1):\n",
    "                if i == 0:\n",
    "                    layer = GraphConvolutionBS(self.in_features, self.hiddendim, self.activation, self.withbn,\n",
    "                                               self.withloop)\n",
    "                else:\n",
    "                    layer = GraphConvolutionBS(self.hiddendim, self.hiddendim, self.activation, self.withbn,\n",
    "                                               self.withloop)\n",
    "                reslayer.append(layer)\n",
    "            self.midlayers.append(reslayer)\n",
    "\n",
    "    def forward(self, input, adj):\n",
    "        x = input\n",
    "        for reslayer in self.midlayers:\n",
    "            subx = input\n",
    "            for gc in reslayer:\n",
    "                subx = gc(subx, adj)\n",
    "                subx = F.dropout(subx, self.dropout, training=self.training)\n",
    "            x = self._doconcat(x, subx)\n",
    "        return x\n",
    "\n",
    "    def get_outdim(self):\n",
    "        return self.out_features\n",
    "\n",
    "    def _doconcat(self, x, subx):\n",
    "        if self.aggrmethod == \"concat\":\n",
    "            return torch.cat((x, subx), 1)\n",
    "        elif self.aggrmethod == \"add\":\n",
    "            return x + subx\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"%s %s (%d - [%d:%d] > %d)\" % (self.__class__.__name__,\n",
    "                                              self.aggrmethod,\n",
    "                                              self.in_features,\n",
    "                                              self.hiddendim,\n",
    "                                              self.nbaselayer,\n",
    "                                              self.out_features)\n",
    "\n",
    "\n",
    "class Dense(Module):\n",
    "    \"\"\"\n",
    "    Simple Dense layer, Do not consider adj.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, activation=lambda x: x, bias=True, res=False):\n",
    "        super(Dense, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.sigma = activation\n",
    "        self.weight = Parameter(torch.FloatTensor(in_features, out_features))\n",
    "        self.res = res\n",
    "        self.bn = nn.BatchNorm1d(out_features)\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.FloatTensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, input, adj):\n",
    "        output = torch.mm(input, self.weight)\n",
    "        if self.bias is not None:\n",
    "            output = output + self.bias\n",
    "        output = self.bn(output)\n",
    "        return self.sigma(output)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' \\\n",
    "               + str(self.in_features) + ' -> ' \\\n",
    "               + str(self.out_features) + ')'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "\n",
    "class GCNModel(nn.Module):\n",
    "    \"\"\"\n",
    "       The model for the single kind of deepgcn blocks.\n",
    "\n",
    "       The model architecture likes:\n",
    "       inputlayer(nfeat)--block(nbaselayer, nhid)--...--outputlayer(nclass)--softmax(nclass)\n",
    "                           |------  nhidlayer  ----|\n",
    "       The total layer is nhidlayer*nbaselayer + 2.\n",
    "       All options are configurable.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 nfeat,\n",
    "                 nhid,\n",
    "                 nclass,\n",
    "                 nhidlayer,\n",
    "                 dropout,\n",
    "                 baseblock=\"mutigcn\",\n",
    "                 inputlayer=\"gcn\",\n",
    "                 outputlayer=\"gcn\",\n",
    "                 nbaselayer=0,\n",
    "                 activation=lambda x: x,\n",
    "                 withbn=True,\n",
    "                 withloop=True,\n",
    "                 aggrmethod=\"add\",\n",
    "                 mixmode=False):\n",
    "        \"\"\"\n",
    "        Initial function.\n",
    "        :param nfeat: the input feature dimension.\n",
    "        :param nhid:  the hidden feature dimension.\n",
    "        :param nclass: the output feature dimension.\n",
    "        :param nhidlayer: the number of hidden blocks.\n",
    "        :param dropout:  the dropout ratio.\n",
    "        :param baseblock: the baseblock type, can be \"mutigcn\", \"resgcn\", \"densegcn\" and \"inceptiongcn\".\n",
    "        :param inputlayer: the input layer type, can be \"gcn\", \"dense\", \"none\".\n",
    "        :param outputlayer: the input layer type, can be \"gcn\", \"dense\".\n",
    "        :param nbaselayer: the number of layers in one hidden block.\n",
    "        :param activation: the activation function, default is ReLu.\n",
    "        :param withbn: using batch normalization in graph convolution.\n",
    "        :param withloop: using self feature modeling in graph convolution.\n",
    "        :param aggrmethod: the aggregation function for baseblock, can be \"concat\" and \"add\". For \"resgcn\", the default\n",
    "                           is \"add\", for others the default is \"concat\".\n",
    "        :param mixmode: enable cpu-gpu mix mode. If true, put the inputlayer to cpu.\n",
    "        \"\"\"\n",
    "        super(GCNModel, self).__init__()\n",
    "        self.mixmode = mixmode\n",
    "        self.dropout = dropout\n",
    "\n",
    "        if baseblock == \"resgcn\":\n",
    "            self.BASEBLOCK = ResGCNBlock\n",
    "        elif baseblock == \"densegcn\":\n",
    "            self.BASEBLOCK = DenseGCNBlock\n",
    "        elif baseblock == \"mutigcn\":\n",
    "            self.BASEBLOCK = MultiLayerGCNBlock\n",
    "        elif baseblock == \"inceptiongcn\":\n",
    "            self.BASEBLOCK = InecptionGCNBlock\n",
    "        else:\n",
    "            raise NotImplementedError(\"Current baseblock %s is not supported.\" % (baseblock))\n",
    "        if inputlayer == \"gcn\":\n",
    "            # input gc\n",
    "            self.ingc = GraphConvolutionBS(nfeat, nhid, activation, withbn, withloop)\n",
    "            baseblockinput = nhid\n",
    "        elif inputlayer == \"none\":\n",
    "            self.ingc = lambda x: x\n",
    "            baseblockinput = nfeat\n",
    "        else:\n",
    "            self.ingc = Dense(nfeat, nhid, activation)\n",
    "            baseblockinput = nhid\n",
    "\n",
    "        outactivation = lambda x: x\n",
    "        if outputlayer == \"gcn\":\n",
    "            self.outgc = GraphConvolutionBS(baseblockinput, nclass, outactivation, withbn, withloop)\n",
    "        # elif outputlayer ==  \"none\": #here can not be none\n",
    "        #    self.outgc = lambda x: x \n",
    "        else:\n",
    "            self.outgc = Dense(nhid, nclass, activation)\n",
    "\n",
    "        # hidden layer\n",
    "        self.midlayer = nn.ModuleList()\n",
    "        # Dense is not supported now.\n",
    "        # for i in xrange(nhidlayer):\n",
    "        for i in range(nhidlayer):\n",
    "            gcb = self.BASEBLOCK(in_features=baseblockinput,\n",
    "                                 out_features=nhid,\n",
    "                                 nbaselayer=nbaselayer,\n",
    "                                 withbn=withbn,\n",
    "                                 withloop=withloop,\n",
    "                                 activation=activation,\n",
    "                                 dropout=dropout,\n",
    "                                 dense=False,\n",
    "                                 aggrmethod=aggrmethod)\n",
    "            self.midlayer.append(gcb)\n",
    "            baseblockinput = gcb.get_outdim()\n",
    "        # output gc\n",
    "        outactivation = lambda x: x  # we donot need nonlinear activation here.\n",
    "        self.outgc = GraphConvolutionBS(baseblockinput, nclass, outactivation, withbn, withloop)\n",
    "\n",
    "        self.reset_parameters()\n",
    "        if mixmode:\n",
    "            self.midlayer = self.midlayer.to(device)\n",
    "            self.outgc = self.outgc.to(device)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        pass\n",
    "\n",
    "    def forward(self, fea, adj):\n",
    "        # input\n",
    "        if self.mixmode:\n",
    "            x = self.ingc(fea, adj.cpu())\n",
    "        else:\n",
    "            x = self.ingc(fea, adj)\n",
    "\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        if self.mixmode:\n",
    "            x = x.to(device)\n",
    "\n",
    "        # mid block connections\n",
    "        # for i in xrange(len(self.midlayer)):\n",
    "        for i in range(len(self.midlayer)):\n",
    "            midgc = self.midlayer[i]\n",
    "            x = midgc(x, adj)\n",
    "        # output, no relu and dropput here.\n",
    "        x = self.outgc(x, adj)\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Modified GCN\n",
    "class GCNFlatRes(nn.Module):\n",
    "    \"\"\"\n",
    "    (Legacy)\n",
    "    \"\"\"\n",
    "    def __init__(self, nfeat, nhid, nclass, withbn, nreslayer, dropout, mixmode=False):\n",
    "        super(GCNFlatRes, self).__init__()\n",
    "\n",
    "        self.nreslayer = nreslayer\n",
    "        self.dropout = dropout\n",
    "        self.ingc = GraphConvolution(nfeat, nhid, F.relu)\n",
    "        self.reslayer = GCFlatResBlock(nhid, nclass, nhid, nreslayer, dropout)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        # stdv = 1. / math.sqrt(self.attention.size(1))\n",
    "        # self.attention.data.uniform_(-stdv, stdv)\n",
    "        # print(self.attention)\n",
    "        pass\n",
    "\n",
    "    def forward(self, input, adj):\n",
    "        x = self.ingc(input, adj)\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = self.reslayer(x, adj)\n",
    "        # x = F.dropout(x, self.dropout, training=self.training)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: hard coding the model path here.\n",
    "folder = \"tmpmodel\"\n",
    "if not os.path.exists(folder):\n",
    "    os.mkdir(folder)\n",
    "\n",
    "class EarlyStopping:\n",
    "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
    "\n",
    "    def __init__(self, datasets=\"tmp\", patience=7, fname=None, clean=False, verbose=False):\n",
    "        \"\"\" \n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "                            Default: 7\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
    "                            Default: False\n",
    "        \"\"\"\n",
    "\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        timstr = datetime.datetime.now().strftime(\"%m%d-%H%M%S\")\n",
    "        if fname is None:\n",
    "            fname = datasets + \"-\" + timstr + \"-\" + self._random_str() + \".pt\"\n",
    "        self.fname = os.path.join(folder, fname)\n",
    "        self.clean = clean\n",
    "    def __call__(self, val_loss, model):\n",
    "\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(\"EarlyStopping counter: %d out of %d\"%(self.counter, self.patience))\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def _random_str(self, randomlength=3):\n",
    "        a = list(string.ascii_letters)\n",
    "        random.shuffle(a)\n",
    "        return ''.join(a[:randomlength])\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        '''Saves model when validation loss decrease.'''\n",
    "        if self.verbose:\n",
    "            print('Validation loss decreased (%.6f --> %.6f).  Saving model ...'%(self.val_loss_min, val_loss))\n",
    "        torch.save(model.state_dict(), self.fname)\n",
    "        self.val_loss_min = val_loss\n",
    "\n",
    "    def load_checkpoint(self):\n",
    "        return  torch.load(self.fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampler:\n",
    "    \"\"\"Sampling the input graph data.\"\"\"\n",
    "    def __init__(self, dataset, data_path=\"data\", task_type=\"full\"):\n",
    "        self.dataset = dataset\n",
    "        self.data_path = data_path\n",
    "        (self.adj,\n",
    "         self.train_adj,\n",
    "         self.features,\n",
    "         self.train_features,\n",
    "         self.labels,\n",
    "         self.idx_train, \n",
    "         self.idx_val,\n",
    "         self.idx_test, \n",
    "         self.degree,\n",
    "         self.learning_type) = data_loader(dataset, data_path, \"NoNorm\", False, task_type)\n",
    "        \n",
    "        #convert some data to torch tensor ---- may be not the best practice here.\n",
    "        self.features = torch.FloatTensor(self.features).float()\n",
    "        self.train_features = torch.FloatTensor(self.train_features).float()\n",
    "        # self.train_adj = self.train_adj.tocsr()\n",
    "\n",
    "        self.labels_torch = torch.LongTensor(self.labels)\n",
    "        self.idx_train_torch = torch.LongTensor(self.idx_train)\n",
    "        self.idx_val_torch = torch.LongTensor(self.idx_val)\n",
    "        self.idx_test_torch = torch.LongTensor(self.idx_test)\n",
    "\n",
    "        # vertex_sampler cache\n",
    "        # where return a tuple\n",
    "        self.pos_train_idx = np.where(self.labels[self.idx_train] == 1)[0]\n",
    "        self.neg_train_idx = np.where(self.labels[self.idx_train] == 0)[0]\n",
    "        # self.pos_train_neighbor_idx = np.where\n",
    "        \n",
    "\n",
    "        self.nfeat = self.features.shape[1]\n",
    "        self.nclass = int(self.labels.max().item() + 1)\n",
    "        self.trainadj_cache = {}\n",
    "        self.adj_cache = {}\n",
    "        #print(type(self.train_adj))\n",
    "        self.degree_p = None\n",
    "\n",
    "    def _preprocess_adj(self, normalization, adj, cuda):\n",
    "        adj_normalizer = fetch_normalization(normalization)\n",
    "        r_adj = adj_normalizer(adj)\n",
    "        r_adj = sparse_mx_to_torch_sparse_tensor(r_adj).float()\n",
    "        if cuda:\n",
    "            r_adj = r_adj.cuda()\n",
    "        return r_adj\n",
    "\n",
    "    def _preprocess_fea(self, fea, cuda):\n",
    "        if cuda:\n",
    "            return fea.cuda()\n",
    "        else:\n",
    "            return fea\n",
    "\n",
    "    def stub_sampler(self, normalization, cuda):\n",
    "        \"\"\"\n",
    "        The stub sampler. Return the original data. \n",
    "        \"\"\"\n",
    "        if normalization in self.trainadj_cache:\n",
    "            r_adj = self.trainadj_cache[normalization]\n",
    "        else:\n",
    "            r_adj = self._preprocess_adj(normalization, self.train_adj, cuda)\n",
    "            self.trainadj_cache[normalization] = r_adj\n",
    "        fea = self._preprocess_fea(self.train_features, cuda)\n",
    "        return r_adj, fea\n",
    "\n",
    "    def randomedge_sampler(self, percent, normalization, cuda):\n",
    "        \"\"\"\n",
    "        Randomly drop edge and preserve percent% edges.\n",
    "        \"\"\"\n",
    "        \"Opt here\"\n",
    "        if percent >= 1.0:\n",
    "            return self.stub_sampler(normalization, cuda)\n",
    "        \n",
    "        nnz = self.train_adj.nnz\n",
    "        perm = np.random.permutation(nnz)\n",
    "        preserve_nnz = int(nnz*percent)\n",
    "        perm = perm[:preserve_nnz]\n",
    "        r_adj = sp.coo_matrix((self.train_adj.data[perm],\n",
    "                               (self.train_adj.row[perm],\n",
    "                                self.train_adj.col[perm])),\n",
    "                              shape=self.train_adj.shape)\n",
    "        r_adj = self._preprocess_adj(normalization, r_adj, cuda)\n",
    "        fea = self._preprocess_fea(self.train_features, cuda)\n",
    "        return r_adj, fea\n",
    "\n",
    "    def vertex_sampler(self, percent, normalization, cuda):\n",
    "        \"\"\"\n",
    "        Randomly drop vertexes.\n",
    "        \"\"\"\n",
    "        if percent >= 1.0:\n",
    "            return self.stub_sampler(normalization, cuda)\n",
    "        self.learning_type = \"inductive\"\n",
    "        pos_nnz = len(self.pos_train_idx)\n",
    "        # neg_neighbor_nnz = 0.4 * percent\n",
    "        neg_no_neighbor_nnz = len(self.neg_train_idx)\n",
    "        pos_perm = np.random.permutation(pos_nnz)\n",
    "        neg_perm = np.random.permutation(neg_no_neighbor_nnz)\n",
    "        pos_perseve_nnz = int(0.9 * percent * pos_nnz)\n",
    "        neg_perseve_nnz = int(0.1 * percent * neg_no_neighbor_nnz)\n",
    "        # print(pos_perseve_nnz)\n",
    "        # print(neg_perseve_nnz)\n",
    "        pos_samples = self.pos_train_idx[pos_perm[:pos_perseve_nnz]]\n",
    "        neg_samples = self.neg_train_idx[neg_perm[:neg_perseve_nnz]]\n",
    "        all_samples = np.concatenate((pos_samples, neg_samples))\n",
    "        r_adj = self.train_adj\n",
    "        r_adj = r_adj[all_samples, :]\n",
    "        r_adj = r_adj[:, all_samples]\n",
    "        r_fea = self.train_features[all_samples, :]\n",
    "        # print(r_fea.shape)\n",
    "        # print(r_adj.shape)\n",
    "        # print(len(all_samples))\n",
    "        r_adj = self._preprocess_adj(normalization, r_adj, cuda)\n",
    "        r_fea = self._preprocess_fea(r_fea, cuda)\n",
    "        return r_adj, r_fea, all_samples\n",
    "\n",
    "    def degree_sampler(self, percent, normalization, cuda):\n",
    "        \"\"\"\n",
    "        Randomly drop edge wrt degree (high degree, low probility).\n",
    "        \"\"\"\n",
    "        if percent >= 0:\n",
    "            return self.stub_sampler(normalization, cuda)\n",
    "        if self.degree_p is None:\n",
    "            degree_adj = self.train_adj.multiply(self.degree)\n",
    "            self.degree_p = degree_adj.data / (1.0 * np.sum(degree_adj.data))\n",
    "        # degree_adj = degree_adj.multi degree_adj.sum()\n",
    "        nnz = self.train_adj.nnz\n",
    "        preserve_nnz = int(nnz * percent)\n",
    "        perm = np.random.choice(nnz, preserve_nnz, replace=False, p=self.degree_p)\n",
    "        r_adj = sp.coo_matrix((self.train_adj.data[perm],\n",
    "                               (self.train_adj.row[perm],\n",
    "                                self.train_adj.col[perm])),\n",
    "                              shape=self.train_adj.shape)\n",
    "        r_adj = self._preprocess_adj(normalization, r_adj, cuda)\n",
    "        fea = self._preprocess_fea(self.train_features, cuda)\n",
    "        return r_adj, fea\n",
    "\n",
    "\n",
    "    def get_test_set(self, normalization, cuda):\n",
    "        \"\"\"\n",
    "        Return the test set. \n",
    "        \"\"\"\n",
    "        if self.learning_type == \"transductive\":\n",
    "            return self.stub_sampler(normalization, cuda)\n",
    "        else:\n",
    "            if normalization in self.adj_cache:\n",
    "                r_adj = self.adj_cache[normalization]\n",
    "            else:\n",
    "                r_adj = self._preprocess_adj(normalization, self.adj, cuda)\n",
    "                self.adj_cache[normalization] = r_adj\n",
    "            fea = self._preprocess_fea(self.features, cuda)\n",
    "            return r_adj, fea\n",
    "\n",
    "    def get_val_set(self, normalization, cuda):\n",
    "        \"\"\"\n",
    "        Return the validataion set. Only for the inductive task.\n",
    "        Currently behave the same with get_test_set\n",
    "        \"\"\"\n",
    "        return self.get_test_set(normalization, cuda)\n",
    "\n",
    "    def get_label_and_idxes(self, cuda):\n",
    "        \"\"\"\n",
    "        Return all labels and indexes.\n",
    "        \"\"\"\n",
    "        if cuda:\n",
    "            return self.labels_torch.cuda(), self.idx_train_torch.cuda(), self.idx_val_torch.cuda(), self.idx_test_torch.cuda()\n",
    "        return self.labels_torch, self.idx_train_torch, self.idx_val_torch, self.idx_test_torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_onehot(labels):\n",
    "    classes = set(labels)\n",
    "    classes_dict = {c: np.identity(len(classes))[i, :] for i, c in\n",
    "                    enumerate(classes)}\n",
    "    labels_onehot = np.array(list(map(classes_dict.get, labels)),\n",
    "                             dtype=np.int32)\n",
    "    return labels_onehot\n",
    "\n",
    "\n",
    "def accuracy(output, labels):\n",
    "    preds = output.max(1)[1].type_as(labels)\n",
    "    correct = preds.eq(labels).double()\n",
    "    correct = correct.sum()\n",
    "    return correct / len(labels)\n",
    "\n",
    "\n",
    "def roc_auc_compute_fn(y_preds, y_targets):\n",
    "    try:\n",
    "        from sklearn.metrics import roc_auc_score\n",
    "    except ImportError:\n",
    "        raise RuntimeError(\"This contrib module requires sklearn to be installed.\")\n",
    "\n",
    "    y_true = y_targets.cpu().numpy()\n",
    "    y_true = encode_onehot(y_true)\n",
    "    y_pred = y_preds.cpu().detach().numpy()\n",
    "    return roc_auc_score(y_true, y_pred)\n",
    "\n",
    "\n",
    "def prec_recall_n(output, labels, topn):\n",
    "    preds = output.detach().numpy()[-1]\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training settings\n",
    "parser = argparse.ArgumentParser()\n",
    "# Training parameter \n",
    "parser.add_argument('--no_cuda', action='store_true', default=False,\n",
    "                    help='Disables CUDA training.')\n",
    "parser.add_argument('--fastmode', action='store_true', default=False,\n",
    "                    help='Disable validation during training.')\n",
    "parser.add_argument('--seed', type=int, default=42, help='Random seed.')\n",
    "parser.add_argument('--epochs', type=int, default=800,\n",
    "                    help='Number of epochs to train.')\n",
    "parser.add_argument('--lr', type=float, default=0.02,\n",
    "                    help='Initial learning rate.')\n",
    "parser.add_argument('--lradjust', action='store_true',\n",
    "                    default=False, help='Enable leraning rate adjust.(ReduceLROnPlateau or Linear Reduce)')\n",
    "parser.add_argument('--weight_decay', type=float, default=5e-4,\n",
    "                    help='Weight decay (L2 loss on parameters).')\n",
    "parser.add_argument(\"--mixmode\", action=\"store_true\",\n",
    "                    default=False, help=\"Enable CPU GPU mixing mode.\")\n",
    "parser.add_argument(\"--warm_start\", default=\"\",\n",
    "                    help=\"The model name to be loaded for warm start.\")\n",
    "parser.add_argument('--debug', action='store_true',\n",
    "                    default=False, help=\"Enable the detialed training output.\")\n",
    "parser.add_argument('--dataset', default=\"cora\", help=\"The data set\")\n",
    "parser.add_argument('--datapath', default=\"data/\", help=\"The data path.\")\n",
    "parser.add_argument(\"--early_stopping\", type=int,\n",
    "                    default=0, help=\"The patience of earlystopping. Do not adopt the earlystopping when it equals 0.\")\n",
    "parser.add_argument(\"--no_tensorboard\", default=False, help=\"Disable writing logs to tensorboard\")\n",
    "\n",
    "# Model parameter\n",
    "parser.add_argument('--type',\n",
    "                    help=\"Choose the model to be trained.(mutigcn, resgcn, densegcn, inceptiongcn)\")\n",
    "parser.add_argument('--inputlayer', default='gcn',\n",
    "                    help=\"The input layer of the model.\")\n",
    "parser.add_argument('--outputlayer', default='gcn',\n",
    "                    help=\"The output layer of the model.\")\n",
    "parser.add_argument('--hidden', type=int, default=128,\n",
    "                    help='Number of hidden units.')\n",
    "parser.add_argument('--dropout', type=float, default=0.5,\n",
    "                    help='Dropout rate (1 - keep probability).')\n",
    "parser.add_argument('--withbn', action='store_true', default=False,\n",
    "                    help='Enable Bath Norm GCN')\n",
    "parser.add_argument('--withloop', action=\"store_true\", default=False,\n",
    "                    help=\"Enable loop layer GCN\")\n",
    "parser.add_argument('--nhiddenlayer', type=int, default=1,\n",
    "                    help='The number of hidden layers.')\n",
    "parser.add_argument(\"--normalization\", default=\"AugNormAdj\",\n",
    "                    help=\"The normalization on the adj matrix.\")\n",
    "parser.add_argument(\"--sampling_percent\", type=float, default=1.0,\n",
    "                    help=\"The percent of the preserve edges. If it equals 1, no sampling is done on adj matrix.\")\n",
    "# parser.add_argument(\"--baseblock\", default=\"res\", help=\"The base building block (resgcn, densegcn, mutigcn, inceptiongcn).\")\n",
    "parser.add_argument(\"--nbaseblocklayer\", type=int, default=1,\n",
    "                    help=\"The number of layers in each baseblock\")\n",
    "parser.add_argument(\"--aggrmethod\", default=\"default\",\n",
    "                    help=\"The aggrmethod for the layer aggreation. The options includes add and concat. Only valid in resgcn, densegcn and inecptiongcn\")\n",
    "parser.add_argument(\"--task_type\", default=\"full\", help=\"The node classification task type (full and semi). Only valid for cora, citeseer and pubmed dataset.\")\n",
    "\n",
    "args = parser.parse_args()\n",
    "if args.debug:\n",
    "    print(args)\n",
    "# pre setting\n",
    "args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "args.mixmode = args.no_cuda and args.mixmode and torch.cuda.is_available()\n",
    "if args.aggrmethod == \"default\":\n",
    "    if args.type == \"resgcn\":\n",
    "        args.aggrmethod = \"add\"\n",
    "    else:\n",
    "        args.aggrmethod = \"concat\"\n",
    "if args.fastmode and args.early_stopping > 0:\n",
    "    args.early_stopping = 0\n",
    "    print(\"In the fast mode, early_stopping is not valid option. Setting early_stopping = 0.\")\n",
    "if args.type == \"mutigcn\":\n",
    "    print(\"For the multi-layer gcn model, the aggrmethod is fixed to nores and nhiddenlayers = 1.\")\n",
    "    args.nhiddenlayer = 1\n",
    "    args.aggrmethod = \"nores\"\n",
    "\n",
    "# random seed setting\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "if args.cuda or args.mixmode:\n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "\n",
    "# should we need fix random seed here?\n",
    "sampler = Sampler(args.dataset, args.datapath, args.task_type)\n",
    "\n",
    "# get labels and indexes\n",
    "labels, idx_train, idx_val, idx_test = sampler.get_label_and_idxes(args.cuda)\n",
    "nfeat = sampler.nfeat\n",
    "nclass = sampler.nclass\n",
    "print(\"nclass: %d\\tnfea:%d\" % (nclass, nfeat))\n",
    "\n",
    "# The model\n",
    "model = GCNModel(nfeat=nfeat,\n",
    "                 nhid=args.hidden,\n",
    "                 nclass=nclass,\n",
    "                 nhidlayer=args.nhiddenlayer,\n",
    "                 dropout=args.dropout,\n",
    "                 baseblock=args.type,\n",
    "                 inputlayer=args.inputlayer,\n",
    "                 outputlayer=args.outputlayer,\n",
    "                 nbaselayer=args.nbaseblocklayer,\n",
    "                 activation=F.relu,\n",
    "                 withbn=args.withbn,\n",
    "                 withloop=args.withloop,\n",
    "                 aggrmethod=args.aggrmethod,\n",
    "                 mixmode=args.mixmode)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(),\n",
    "                       lr=args.lr, weight_decay=args.weight_decay)\n",
    "\n",
    "# scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=50, factor=0.618)\n",
    "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[200, 300, 400, 500, 600, 700], gamma=0.5)\n",
    "# convert to cuda\n",
    "if args.cuda:\n",
    "    model.cuda()\n",
    "\n",
    "# For the mix mode, lables and indexes are in cuda. \n",
    "if args.cuda or args.mixmode:\n",
    "    labels = labels.cuda()\n",
    "    idx_train = idx_train.cuda()\n",
    "    idx_val = idx_val.cuda()\n",
    "    idx_test = idx_test.cuda()\n",
    "\n",
    "if args.warm_start is not None and args.warm_start != \"\":\n",
    "    early_stopping = EarlyStopping(fname=args.warm_start, verbose=False)\n",
    "    print(\"Restore checkpoint from %s\" % (early_stopping.fname))\n",
    "    model.load_state_dict(early_stopping.load_checkpoint())\n",
    "\n",
    "# set early_stopping\n",
    "if args.early_stopping > 0:\n",
    "    early_stopping = EarlyStopping(patience=args.early_stopping, verbose=False)\n",
    "    print(\"Model is saving to: %s\" % (early_stopping.fname))\n",
    "\n",
    "if args.no_tensorboard is False:\n",
    "    tb_writer = SummaryWriter(\n",
    "        comment=f\"-dataset_{args.dataset}-type_{args.type}\"\n",
    "    )\n",
    "\n",
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']\n",
    "\n",
    "\n",
    "# define the training function.\n",
    "def train(epoch, train_adj, train_fea, idx_train, val_adj=None, val_fea=None):\n",
    "    if val_adj is None:\n",
    "        val_adj = train_adj\n",
    "        val_fea = train_fea\n",
    "\n",
    "    t = time.time()\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(train_fea, train_adj)\n",
    "    # special for reddit\n",
    "    if sampler.learning_type == \"inductive\":\n",
    "        loss_train = F.nll_loss(output, labels[idx_train])\n",
    "        acc_train = accuracy(output, labels[idx_train])\n",
    "    else:\n",
    "        loss_train = F.nll_loss(output[idx_train], labels[idx_train])\n",
    "        acc_train = accuracy(output[idx_train], labels[idx_train])\n",
    "\n",
    "    loss_train.backward()\n",
    "    optimizer.step()\n",
    "    train_t = time.time() - t\n",
    "    val_t = time.time()\n",
    "    # We can not apply the fastmode for the reddit dataset.\n",
    "    # if sampler.learning_type == \"inductive\" or not args.fastmode:\n",
    "\n",
    "    if args.early_stopping > 0 and sampler.dataset != \"reddit\":\n",
    "        loss_val = F.nll_loss(output[idx_val], labels[idx_val]).item()\n",
    "        early_stopping(loss_val, model)\n",
    "\n",
    "    if not args.fastmode:\n",
    "        #    # Evaluate validation set performance separately,\n",
    "        #    # deactivates dropout during validation run.\n",
    "        model.eval()\n",
    "        output = model(val_fea, val_adj)\n",
    "        loss_val = F.nll_loss(output[idx_val], labels[idx_val]).item()\n",
    "        acc_val = accuracy(output[idx_val], labels[idx_val]).item()\n",
    "        if sampler.dataset == \"reddit\":\n",
    "            early_stopping(loss_val, model)\n",
    "    else:\n",
    "        loss_val = 0\n",
    "        acc_val = 0\n",
    "\n",
    "    if args.lradjust:\n",
    "        scheduler.step()\n",
    "\n",
    "    val_t = time.time() - val_t\n",
    "    return (loss_train.item(), acc_train.item(), loss_val, acc_val, get_lr(optimizer), train_t, val_t)\n",
    "\n",
    "\n",
    "def test(test_adj, test_fea):\n",
    "    model.eval()\n",
    "    output = model(test_fea, test_adj)\n",
    "    loss_test = F.nll_loss(output[idx_test], labels[idx_test])\n",
    "    acc_test = accuracy(output[idx_test], labels[idx_test])\n",
    "    auc_test = roc_auc_compute_fn(output[idx_test], labels[idx_test])\n",
    "    if args.debug:\n",
    "        print(\"Test set results:\",\n",
    "              \"loss= {:.4f}\".format(loss_test.item()),\n",
    "              \"auc= {:.4f}\".format(auc_test),\n",
    "              \"accuracy= {:.4f}\".format(acc_test.item()))\n",
    "        print(\"accuracy=%.5f\" % (acc_test.item()))\n",
    "    return (loss_test.item(), acc_test.item())\n",
    "\n",
    "\n",
    "# Train model\n",
    "t_total = time.time()\n",
    "loss_train = np.zeros((args.epochs,))\n",
    "acc_train = np.zeros((args.epochs,))\n",
    "loss_val = np.zeros((args.epochs,))\n",
    "acc_val = np.zeros((args.epochs,))\n",
    "\n",
    "sampling_t = 0\n",
    "\n",
    "for epoch in range(args.epochs):\n",
    "    input_idx_train = idx_train\n",
    "    sampling_t = time.time()\n",
    "    # no sampling\n",
    "    # randomedge sampling if args.sampling_percent >= 1.0, it behaves the same as stub_sampler.\n",
    "    (train_adj, train_fea) = sampler.randomedge_sampler(percent=args.sampling_percent, normalization=args.normalization,\n",
    "                                                        cuda=args.cuda)\n",
    "    if args.mixmode:\n",
    "        train_adj = train_adj.cuda()\n",
    "\n",
    "    sampling_t = time.time() - sampling_t\n",
    "    \n",
    "    # The validation set is controlled by idx_val\n",
    "    # if sampler.learning_type == \"transductive\":\n",
    "    if False:\n",
    "        outputs = train(epoch, train_adj, train_fea, input_idx_train)\n",
    "    else:\n",
    "        (val_adj, val_fea) = sampler.get_test_set(normalization=args.normalization, cuda=args.cuda)\n",
    "        if args.mixmode:\n",
    "            val_adj = val_adj.cuda()\n",
    "        outputs = train(epoch, train_adj, train_fea, input_idx_train, val_adj, val_fea)\n",
    "\n",
    "    if args.debug and epoch % 1 == 0:\n",
    "        print('Epoch: {:04d}'.format(epoch + 1),\n",
    "              'loss_train: {:.4f}'.format(outputs[0]),\n",
    "              'acc_train: {:.4f}'.format(outputs[1]),\n",
    "              'loss_val: {:.4f}'.format(outputs[2]),\n",
    "              'acc_val: {:.4f}'.format(outputs[3]),\n",
    "              'cur_lr: {:.5f}'.format(outputs[4]),\n",
    "              's_time: {:.4f}s'.format(sampling_t),\n",
    "              't_time: {:.4f}s'.format(outputs[5]),\n",
    "              'v_time: {:.4f}s'.format(outputs[6]))\n",
    "    \n",
    "    if args.no_tensorboard is False:\n",
    "        tb_writer.add_scalars('Loss', {'train': outputs[0], 'val': outputs[2]}, epoch)\n",
    "        tb_writer.add_scalars('Accuracy', {'train': outputs[1], 'val': outputs[3]}, epoch)\n",
    "        tb_writer.add_scalar('lr', outputs[4], epoch)\n",
    "        tb_writer.add_scalars('Time', {'train': outputs[5], 'val': outputs[6]}, epoch)\n",
    "        \n",
    "\n",
    "    loss_train[epoch], acc_train[epoch], loss_val[epoch], acc_val[epoch] = outputs[0], outputs[1], outputs[2], outputs[\n",
    "        3]\n",
    "\n",
    "    if args.early_stopping > 0 and early_stopping.early_stop:\n",
    "        print(\"Early stopping.\")\n",
    "        model.load_state_dict(early_stopping.load_checkpoint())\n",
    "        break\n",
    "\n",
    "if args.early_stopping > 0:\n",
    "    model.load_state_dict(early_stopping.load_checkpoint())\n",
    "\n",
    "if args.debug:\n",
    "    print(\"Optimization Finished!\")\n",
    "    print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
    "\n",
    "# Testing\n",
    "(test_adj, test_fea) = sampler.get_test_set(normalization=args.normalization, cuda=args.cuda)\n",
    "if args.mixmode:\n",
    "    test_adj = test_adj.cuda()\n",
    "(loss_test, acc_test) = test(test_adj, test_fea)\n",
    "print(\"%.6f\\t%.6f\\t%.6f\\t%.6f\\t%.6f\\t%.6f\" % (\n",
    "loss_train[-1], loss_val[-1], loss_test, acc_train[-1], acc_val[-1], acc_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
